<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>YOLO V1 论文精读 | Self-Improvement</title><meta name="author" content="Vane"><meta name="copyright" content="Vane"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="首先补充一些常识 VOC数据集具有20个类别    MS COCO数据集具有80个类别 RCN对于背景信息具有更高的误判，区分背景和物体的能力差 YOLO对于背景信息和物体信息具有更好的理解，但是物体定位精读差 Abstract   单词 意义    prior 先前的   repurpose 重复使用   regression 回归   spatially 空间的   associated 相关">
<meta property="og:type" content="article">
<meta property="og:title" content="YOLO V1 论文精读">
<meta property="og:url" content="http://example.com/2025/03/04/yolov1%20-%20%E8%AE%BA%E6%96%87/index.html">
<meta property="og:site_name" content="Self-Improvement">
<meta property="og:description" content="首先补充一些常识 VOC数据集具有20个类别    MS COCO数据集具有80个类别 RCN对于背景信息具有更高的误判，区分背景和物体的能力差 YOLO对于背景信息和物体信息具有更好的理解，但是物体定位精读差 Abstract   单词 意义    prior 先前的   repurpose 重复使用   regression 回归   spatially 空间的   associated 相关">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/touxiang1.jpg">
<meta property="article:published_time" content="2025-03-04T07:41:37.036Z">
<meta property="article:modified_time" content="2025-03-17T13:09:09.149Z">
<meta property="article:author" content="Vane">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="论文精读">
<meta property="article:tag" content="目标检测">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/touxiang1.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "YOLO V1 论文精读",
  "url": "http://example.com/2025/03/04/yolov1%20-%20%E8%AE%BA%E6%96%87/",
  "image": "http://example.com/img/touxiang1.jpg",
  "datePublished": "2025-03-04T07:41:37.036Z",
  "dateModified": "2025-03-17T13:09:09.149Z",
  "author": [
    {
      "@type": "Person",
      "name": "Vane",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/03/04/yolov1%20-%20%E8%AE%BA%E6%96%87/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'YOLO V1 论文精读',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/transpancy.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/bg1.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/touxiang1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Self-Improvement</span></a><a class="nav-page-title" href="/"><span class="site-name">YOLO V1 论文精读</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">YOLO V1 论文精读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-04T07:41:37.036Z" title="发表于 2025-03-04 15:41:37">2025-03-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-17T13:09:09.149Z" title="更新于 2025-03-17 21:09:09">2025-03-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">论文精读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div class="top-img gist" style="background-image: url(/null)"></div><article class="post-content" id="article-container"><p>首先补充一些常识</p>
<p>VOC数据集具有20个类别    MS COCO数据集具有80个类别</p>
<p>RCN对于背景信息具有更高的误判，区分背景和物体的能力差</p>
<p>YOLO对于背景信息和物体信息具有更好的理解，但是物体定位精读差</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>prior</td>
<td>先前的</td>
</tr>
<tr>
<td>repurpose</td>
<td>重复使用</td>
</tr>
<tr>
<td>regression</td>
<td>回归</td>
</tr>
<tr>
<td>spatially</td>
<td>空间的</td>
</tr>
<tr>
<td>associated</td>
<td>相关的</td>
</tr>
<tr>
<td>unified</td>
<td>统一的</td>
</tr>
<tr>
<td>astounding</td>
<td>令人震惊的</td>
</tr>
<tr>
<td>general</td>
<td>普适的</td>
</tr>
<tr>
<td>generalizing</td>
<td>迁移泛化</td>
</tr>
<tr>
<td>domain</td>
<td>领域</td>
</tr>
</tbody></table>
<p>之前的工作都是两阶段目标检测的方法</p>
<p>YOLO将其变为了预测连续数值的regression回归问题，将其在空间上把边界框和分类概率分离了</p>
<p>是全图输入的模型，实现了端到端</p>
<p>分类成功但是定位差，不过具有更低的把背景信息误判成物体的概率</p>
<p>具有良好的迁移能力可以迁移到其他领域</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>instantly</td>
<td>立刻</td>
</tr>
<tr>
<td>assistive</td>
<td>协助</td>
</tr>
<tr>
<td>sensor</td>
<td>传感器</td>
</tr>
<tr>
<td>robotic</td>
<td>机器人的</td>
</tr>
<tr>
<td>evenly</td>
<td>平均地</td>
</tr>
<tr>
<td>potential</td>
<td>潜在的</td>
</tr>
<tr>
<td>post-processing</td>
<td>后处理</td>
</tr>
<tr>
<td>reframe</td>
<td>重构</td>
</tr>
<tr>
<td>simultaneously</td>
<td>同时地</td>
</tr>
<tr>
<td>unified</td>
<td>统一的</td>
</tr>
<tr>
<td>latency</td>
<td>延迟</td>
</tr>
<tr>
<td>reason</td>
<td>分析</td>
</tr>
<tr>
<td>implicitly</td>
<td>隐式地</td>
</tr>
<tr>
<td>context</td>
<td>上下文</td>
</tr>
<tr>
<td>generalizable</td>
<td>泛化的</td>
</tr>
<tr>
<td>representation</td>
<td>特征表示</td>
</tr>
<tr>
<td>by a wide margin</td>
<td>大幅度</td>
</tr>
<tr>
<td>unexpected input</td>
<td>和之前分布不同的输入图像</td>
</tr>
<tr>
<td>it struggles to</td>
<td>难以</td>
</tr>
<tr>
<td>tradeoff</td>
<td>权衡</td>
</tr>
</tbody></table>
<p>当前(对于论文的当前)的目标检测系统都重复使用分类器(不同位置不同尺度的重复使用，类似于滑动窗口)</p>
<p>都是两阶段分类器-&gt;①提取潜在的候选框(region proposal) ②用分类器筛选每一个候选框 ③然后使用非极大值抑制之后微调，然后重新给每个预测框进行打分 这些都是分开进行的，每一个内容都要分别进行训练非常麻烦</p>
<p>YOLO 实现了直接从输入图像的像素空间到检测结果的映射，既可以给出分类也可以给出预测框的坐标，同时能输出多个预测框和类别</p>
<p>YOLO的优势 </p>
<ol>
<li>快 -&gt;把目标检测问题重构为了回归问题，不需要太复杂的流水线，同时能输出多个预测框和类别</li>
<li>能捕捉上下文信息 -&gt;滑动窗口和基于proposal方法只能对候选框区域的信息进行处理，但yolo是一个全局视角-&gt;Fast R-CNN会对背景有更高的误判概率 </li>
<li>迁移泛化能力好-&gt;在更换其他的输入(改变输入的分布的情况下)性能下降的概率更小</li>
</ol>
<p>YOLO的缺点-&gt;准确率低，对于小目标定位差</p>
<p><img src="/../blogimg/image-20250304192525331.png" alt="image-20250304192525331"></p>
<p>①缩放图片 ②卷积神经网络 ③后处理</p>
<h1 id="2-Unified-Detection"><a href="#2-Unified-Detection" class="headerlink" title="2. Unified Detection"></a>2. Unified Detection</h1><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>unify</td>
<td>整合</td>
</tr>
<tr>
<td>component</td>
<td>组件</td>
</tr>
<tr>
<td>simultaneously</td>
<td>同时地</td>
</tr>
<tr>
<td>maintain</td>
<td>保持</td>
</tr>
<tr>
<td>formally</td>
<td>形式上</td>
</tr>
<tr>
<td>conditional class probabilities</td>
<td>类别条件概率</td>
</tr>
<tr>
<td>class-specific</td>
<td>各个类别</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>YOLO </p>
<p>→能预测全图每个框的类别→其具备挖掘全幅图片信息的能力</p>
<p>→实现端到端，实时的目标检测</p>
<p>→将图片分成S × S 个grid cell 每个 grid cell生成B个预测框</p>
<p>→ground truth(我们的人工标注框)的中心点落到哪一个grid cell中哪一个grid cell 产生的预测框bounding box来拟合ground truth→grid cell产生的预测框哪一个和ground truth的IOU更大则由哪一个框来进行预测这个ground truth</p>
<p>→每个bounding box都会有个置信度分数(与是否包含物体的概率以及离标注框的距离又多近) 其值为<img src="/../blogimg/image-20250304194528391.png" alt="image-20250304194528391">，两者成绩即为confidence的标签值，其中Pr(Object)非0即1如果框里包含物体就是1，IOU就是ground truth和预测框的交并比→如果没有物体的bounding box 的分数应该越低越好为0，负责预测的bounding box 的confidence score应该为ground truth和预测框的交并比→对于预测阶段confidence score回归出多少就是多少不需要计算这两个值，只是隐式的包含两者</p>
<p>→每个bounding box都具有五个参数(x,y,w,h,c) </p>
<p>(x,y)是中心点对于grid cell左上角的坐标 所以是∈(0,1)</p>
<p>(h,w)分别是相对于整幅图的宽高也是∈(0,1)</p>
<p>c 每个框对应的置信度得分</p>
<p>C 已包含物体的情况下各类别的概率  (每个grid cell 的所有预测bounding box 共享后面的C每个类别对应的条件概率)</p>
<p>→因此每一个grid cell有B × 5 + C 个数值</p>
<p><img src="/../blogimg/image-20250304211351537.png" alt="image-20250304211351537"></p>
<p>每个框的置信分数乘其包含物体情况下各类别的概率即可得到这个预测框对应的预测物体的真实概率，再乘与ground truth的IOU得到得分</p>
<p><strong>其他进一步讲解可以左转进入YOLO v1 技术剖析</strong></p>
<p><img src="/../blogimg/image-20250306193657819.png" alt="image-20250306193657819"></p>
<h2 id="2-1-Network-Design"><a href="#2-1-Network-Design" class="headerlink" title="2.1. Network Design"></a>2.1. Network Design</h2><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>implement</td>
<td>实现</td>
</tr>
<tr>
<td>extract</td>
<td>提取(特征)</td>
</tr>
<tr>
<td>coordinate</td>
<td>坐标</td>
</tr>
<tr>
<td>other than</td>
<td>除了</td>
</tr>
</tbody></table>
<p>使用了 PASCAL VOC detection dataset</p>
<p>受到GoogleNet的启发但是没有用inception模块而是简单的3×3卷积后面跟着1×1卷积</p>
<p>Fast YOLO 和YOLO的区别是 更少的卷积层(9个) 更少的卷积核，其他的内容都和YOLO相同</p>
<p><img src="/../blogimg/image-20250306193711146.png" alt="image-20250306193711146"></p>
<p>交替的1×1卷积来减少特征空间 使用224×224×3的输入图像(ImageNet)来训练图像分类 用两倍448×448×3的图像来训练目标检测</p>
<h2 id="2-2-Training"><a href="#2-2-Training" class="headerlink" title="2.2. Training"></a>2.2. Training</h2><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>comparable</td>
<td>相当的</td>
</tr>
<tr>
<td>resolution</td>
<td>分辨率</td>
</tr>
<tr>
<td>fine-grained</td>
<td>细粒度</td>
</tr>
<tr>
<td>parametrize</td>
<td>参数化</td>
</tr>
<tr>
<td>bounding box coordinates</td>
<td>框定位回归</td>
</tr>
<tr>
<td>leaky</td>
<td>泄露</td>
</tr>
<tr>
<td>sum-squared</td>
<td>平方和误差</td>
</tr>
<tr>
<td>align</td>
<td>对齐</td>
</tr>
<tr>
<td>ideal</td>
<td>适当的</td>
</tr>
<tr>
<td>overpower</td>
<td>压倒</td>
</tr>
<tr>
<td>instability</td>
<td>不确定</td>
</tr>
<tr>
<td>diverge</td>
<td>发散</td>
</tr>
<tr>
<td>remedy</td>
<td>补偿</td>
</tr>
<tr>
<td>partially</td>
<td>部分</td>
</tr>
<tr>
<td>assign</td>
<td>指定</td>
</tr>
<tr>
<td>specialization</td>
<td>特化</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>预训练 在ImageNet 1000-class数据集上训练了前二十个的卷积层(后面跟着一个全局池化AVG 以及一个全连接层)</p>
<p>根据Fast R-CNN的启发，后面加卷积层和全连接层能提升性能因此加了四个卷积层和两个全连接层(随机初始化)</p>
<p>因为目标检测是一个细粒度的视觉任务，因此将图片的分辨率提升到448×448</p>
<p>最后一层预测了类别概率和框定位坐标，并且对于坐标还有框长宽进行了归一化</p>
<p>最后一层的激活函数是线性激活函数，其他层用的全是leaky relu函数</p>
<p><img src="/../blogimg/image-20250306201834027.png" alt="image-20250306201834027"></p>
<p>使用的是优化的平方和误差损失函数(回归问题用的损失函数)</p>
<p>如果我们给不用来预测物体的框或者本身grid cell中就没落入ground truth中心点的框和用来预测物体的框具有相同的权重，那样会导致训练的发散和模型的不稳定</p>
<p>为了补偿对于定位的框的损失，我们应该加强定位误差的损失而削弱不博阿寒ground truth的预测框的confidence 损失</p>
<p>λcoord &#x3D; 5            λnoobj &#x3D; 0.5</p>
<p>大框中的小偏差比小框中的小偏差更不重要。为了部分解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度</p>
<p>ground truth的中心点落入哪一个grid cell里面，那哪一个grid cell 预测出的其中一个bounding box 拟合这个ground truth ，这个bounding box 的选取是选取与ground truth 的IOU最大的bounding box，另一个bounding box需要降低置信度</p>
<p>每个框都会逐渐的特化，类似于有些就是用来预测自行车有些就是用来预测行人这种</p>
<p><img src="/../blogimg/image-20250306204518192.png" alt="image-20250306204518192"></p>
<p><strong>损失函数具体解释左转技术文档</strong></p>
<p>在 PASCAL VOC 2007 和 2012 的训练和验证数据集上训练网络大约 135 个epoch。</p>
<p>在 2012 上进行测试时，还包括 VOC 2007 测试数据以进行训练。</p>
<p>在整个训练过程中，使用 64 的批量大小、0.9 的动量和 0.0005 的衰减</p>
<p>训练过程先学习率将学习率从 10-3 提高到 10-2。如果以高学习率开始，模型经常会因梯度不稳定而发散。继续用 10-2 训练 75 个epoch，然后是 10-3 训练 30 个epoch，最后是 10-4 训练 30 个epoch</p>
<p>使用drop out 和数据增强来繁殖过拟合  </p>
<p>dropout概率为0.5</p>
<p>随机缩放和平移，最高可达原始图像大小的 20%，在 HSV 色彩空间中随机调整图像的曝光和饱和度，最高可达 1.5 倍。</p>
<p>HSV 色调 饱和度 明度</p>
<h2 id="2-3-Inference"><a href="#2-3-Inference" class="headerlink" title="2.3. Inference"></a>2.3. Inference</h2><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>classifier-based method</td>
<td>基于分类器的方法</td>
</tr>
<tr>
<td>diversity</td>
<td>多样性</td>
</tr>
<tr>
<td>critical to</td>
<td>对…至关重要</td>
</tr>
</tbody></table>
<p>与训练过程类似，测试图像的检测预测只需进行一次网络评估。在 PASCAL VOC 数据集上，网络每张图像可预测 98 个边界框以及每个边界框的类别概率。由于只需一次网络评估，相比基于分类器的方法，YOLO 在测试时速度极快</p>
<p>网格设计在边界框预测中引入了空间多样性。通常，物体所属的网格单元很明确，网络对每个物体仅预测一个边界框。然而，一些大型物体或位于多个网格单元边界附近的物体，可能会被多个网格单元检测到。此时，非极大值抑制可用于处理这些重复检测的问题。虽然它对 YOLO 性能的重要性不如对 R-CNN 或 DPM 那么关键，但能使 mAP 提升</p>
<h2 id="2-4-Limitations-of-YOLO"><a href="#2-4-Limitations-of-YOLO" class="headerlink" title="2.4. Limitations of YOLO"></a>2.4. Limitations of YOLO</h2><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>coarse</td>
<td>粗糙的</td>
</tr>
<tr>
<td>downsampling</td>
<td>下采样</td>
</tr>
<tr>
<td>approximate</td>
<td>近似，使接近</td>
</tr>
<tr>
<td>versus</td>
<td>与…相比</td>
</tr>
</tbody></table>
<p><strong>空间约束限制检测数量</strong>：YOLO对边界框的预测存在较强的空间约束，每个网格单元仅能预测两个边界框且只能有一个类别。这种限制使得模型在预测相邻物体时数量受限，难以检测成群出现的小物体，例如一群鸟。 </p>
<p> <strong>难以适应新的物体形态</strong>：由于模型是从数据中学习预测边界框，对于新的或不常见长宽比及配置的物体，难以进行有效的泛化检测。 </p>
<p> <strong>特征粗糙影响定位精度</strong>：YOLO的架构包含多个对输入图像的下采样层，导致用于预测边界框的特征相对粗糙。</p>
<p>损失函数对大小边界框的误差处理方式相同，然而小边界框中的小误差对交并比（IOU）影响更大，因此模型的主要误差来源是定位不准确。 </p>
<h1 id="3-Comparison-to-Other-Detection-Systems"><a href="#3-Comparison-to-Other-Detection-Systems" class="headerlink" title="3. Comparison to Other Detection Systems"></a>3. Comparison to Other Detection Systems</h1><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>subset</td>
<td>子集</td>
</tr>
<tr>
<td>deformable</td>
<td>可变的，可弯曲的</td>
</tr>
<tr>
<td>contextual</td>
<td>上下文的</td>
</tr>
<tr>
<td>variant</td>
<td>变种，变体</td>
</tr>
<tr>
<td>jointly</td>
<td>共同的，联合的</td>
</tr>
<tr>
<td>cascade</td>
<td>串联</td>
</tr>
<tr>
<td>disjoint</td>
<td>分离的</td>
</tr>
</tbody></table>
<p>第3节主要将YOLO检测系统与其他检测框架进行对比</p>
<p> <strong>与可变形部件模型（DPM）对比</strong>：DPM采用滑动窗口方式，通过独立的流程来提取静态特征、分类区域和预测边界框。而YOLO使用单个卷积神经网络替代这些分散的部分，网络能同时进行特征提取、边界框预测、非极大值抑制和上下文推理，并且训练并优化特征以适应检测任务，相比DPM，速度更快、准确性更高。—》特征提取的内容对比</p>
<p><strong>与R-CNN对比</strong>：R-CNN及其变体通过区域提议寻找图像中的物体，其复杂的管道包含多个独立调整的阶段，导致速度较慢。YOLO与R-CNN有相似之处，如每个网格单元都会提议潜在的边界框并使用卷积特征进行评分。但YOLO对网格单元提议施加了空间约束，减少了对同一物体的重复检测，提议的边界框数量也更少，且将各个组件整合为一个联合优化的模型。 –》生成特别多候选框的对比</p>
<p><strong>与其他快速检测器对比</strong>：Fast和Faster R-CNN致力于加速R-CNN框架，虽在速度和准确性上有提升，但仍未达到实时性能。许多针对DPM管道加速的研究中，仅30Hz DPM能真正实现实时检测。YOLO则完全摒弃传统检测管道，从设计上实现了快速检测。 </p>
<p><strong>与特定类别的检测器对比</strong>：像针对人脸或人的单类检测器，因处理的变化较少可进行高度优化。而YOLO是通用检测器，能同时学习检测多种物体。</p>
<p><strong>与Deep MultiBox对比</strong>：Deep MultiBox训练卷积神经网络来预测感兴趣区域，只能执行单目标检测，且是大型检测管道的一部分，需要进一步对图像补丁进行分类。YOLO则是完整的检测系统。 </p>
<p><strong>与OverFeat对比</strong>：OverFeat通过训练卷积神经网络进行定位并应用于检测，它是一个独立的系统，优化目标是定位而非检测性能，在预测时只能看到局部信息，需要大量后处理来生成连贯的检测结果。而YOLO能全局推理，无需复杂后处理。 </p>
<p><strong>与MultiGrasp对比</strong>：YOLO的网格预测边界框方法受MultiGrasp系统启发，但MultiGrasp仅需预测单个可抓取区域，而YOLO要预测多物体多类别的边界框和类别概率，任务更复杂。 </p>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><h2 id="4-1-Comparison-to-Other-Real-Time-Systems"><a href="#4-1-Comparison-to-Other-Real-Time-Systems" class="headerlink" title="4.1. Comparison to Other Real-Time Systems"></a>4.1. Comparison to Other Real-Time Systems</h2><p>许多目标检测研究致力于加速标准检测流程，但多数未达实时标准</p>
<p>Fast YOLO 是 PASCAL 数据集上已知最快的目标检测方法，mAP 达 52.7%，准确率是此前实时检测方法的两倍多。YOLO 在保持实时性能的同时，将 mAP 提升至 63.4%。</p>
<p>作者训练了基于 VGG - 16 的 YOLO 模型，其精度更高，但速度明显慢于普通 YOLO，不满足实时检测要求，主要用于与其他依赖 VGG - 16 的检测系统作对比。</p>
<p>Fastest DPM 在提升 DPM 速度的同时，未过多牺牲 mAP，但仍未达到实时性能，且与基于神经网络的方法相比，DPM 检测精度较低。</p>
<p>R - CNN minus R 用静态边界框提议替代 Selective Search，虽比 R - CNN 快，但仍未实现实时检测，且因提议质量不佳，精度大幅下降。</p>
<p>Fast R - CNN 加快了 R - CNN 的分类阶段，但仍依赖 Selective Search 生成边界框提议，每张图像生成提议耗时约 2 秒，导致其 mAP 虽高，但帧率仅 0.5fps，远未达到实时标准。</p>
<p>Faster R - CNN 用神经网络替代 Selective Search 来提议边界框，最精确的模型帧率为 7fps，较小且精度稍低的模型帧率为 18fps。VGG - 16 版本的 Faster R - CNN 比 YOLO 的 mAP 高 10，但速度慢 6 倍；ZeilerFergus 版本的 Faster R - CNN 比 YOLO 慢 2.5 倍，且精度更低。</p>
<h2 id="4-2-VOC-2007-Error-Analysis"><a href="#4-2-VOC-2007-Error-Analysis" class="headerlink" title="4.2. VOC 2007 Error Analysis"></a>4.2. VOC 2007 Error Analysis</h2><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>breakdown</td>
<td>细分；详细分类</td>
</tr>
<tr>
<td>methodology</td>
<td>方法体系；方法论；一套方法</td>
</tr>
</tbody></table>
<p>YOLO 定位误差约为 Fast R-CNN 两倍，但其背景误报少，误分类也相对较少</p>
<p>Fast R-CNN 定位精准，但是背景误报问题突出。</p>
<h2 id="4-3-Combining-Fast-R-CNN-and-YOLO"><a href="#4-3-Combining-Fast-R-CNN-and-YOLO" class="headerlink" title="4.3. Combining Fast R-CNN and YOLO"></a>4.3. Combining Fast R-CNN and YOLO</h2><p>主要在 PASCAL VOC 2012 数据集上评估 YOLO 性能。与其他先进方法相比，普通 YOLO 的平均精度均值（mAP）为 63.3%，虽未达顶尖水平，但在保持实时性能的同时，有一定竞争力。基于 VGG-16 的 YOLO 模型精度更高，达 66.4%，但速度较慢。</p>
<p>YOLO 在复杂场景下检测的局限性主要是在一些类别上的检测困难。例如在检测小物体类别（如瓶子）时，YOLO 的性能不如其他一些方法。这反映出 YOLO 对于小物体检测可能存在精度不高的问题，在复杂场景下小物体容易被遗漏或者检测不准确</p>
<h2 id="4-4-VOC-2012-Results"><a href="#4-4-VOC-2012-Results" class="headerlink" title="4.4. VOC 2012 Results"></a>4.4. VOC 2012 Results</h2><p>VOC 2007 数据集上的表现类似，普通 YOLO 模型达到了 63.3% 的平均精度均值（mAP），展示出了实时检测能力下的一定竞争力。基于 VGG-16 的 YOLO 模型精度更高，提升至 66.4%，但速度有所下降。作者通过与其他先进检测方法对比各类别检测结果，发现 YOLO 在部分类别（如小物体类别瓶子）上表现欠佳，反映出其在复杂场景下对小物体检测存在局限性</p>
<h2 id="4-5-Generalizability-Person-Detection-in-Artwork"><a href="#4-5-Generalizability-Person-Detection-in-Artwork" class="headerlink" title="4.5. Generalizability: Person Detection in Artwork"></a>4.5. Generalizability: Person Detection in Artwork</h2><p>选用了两个艺术作品数据集，与其他检测器对比。结果显示，相比其他方法，YOLO 能更好地适应新领域。其原因在于，YOLO 将目标检测视为回归问题，从整幅图像中学习特征，这使它对目标的整体理解更优，不易受特定数据集偏差影响。例如在艺术作品数据集中，其他检测器因数据分布变化性能大幅下降，而 YOLO 仍能保持一定检测能力，充分证明了 YOLO 在新领域具有出色的泛化性。</p>
<h1 id="5-Real-Time-Detection-In-The-Wild"><a href="#5-Real-Time-Detection-In-The-Wild" class="headerlink" title="5. Real-Time Detection In The Wild"></a>5. Real-Time Detection In The Wild</h1><table>
<thead>
<tr>
<th>单词</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>webcam</td>
<td>网络摄像机</td>
</tr>
</tbody></table>
<p>简而言之，在真实场景下YOLO的实时性和准确性也得到了验证</p>
<h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h1><p>YOLO，这是一种用于目标检测的统一模型。我们的模型构建简单，并且可以直接在完整图像上进行训练。与基于分类器的方法不同，YOLO 依据直接与检测性能相关的损失函数进行训练，并且整个模型是联合训练的。</p>
<p>Fast YOLO 是文献记载中速度最快的通用目标检测器，而 YOLO 则推动了实时目标检测领域的技术发展。YOLO 在新领域也具有良好的泛化能力，使其成为依赖快速、稳健目标检测应用的理想选择。</p>
<h1 id="纸质版阅读记录"><a href="#纸质版阅读记录" class="headerlink" title="纸质版阅读记录"></a>纸质版阅读记录</h1><p> <img src="/../blogimg/a4715f3dd493d1f743054d7b5d70c0c.jpg" alt="a4715f3dd493d1f743054d7b5d70c0c"></p>
<p><img src="/../blogimg/7f939e5b976ef4537edc6965e4242a3.jpg" alt="7f939e5b976ef4537edc6965e4242a3"></p>
<p><img src="/../blogimg/a11d35897e174b684b725f86a742273.jpg" alt="a11d35897e174b684b725f86a742273"></p>
<p><img src="/../blogimg/52d5e726b229a799805295a420c275d.jpg" alt="52d5e726b229a799805295a420c275d"><img src="/../blogimg/7f04b849747ae328fad6cd5e63fa9cf.jpg" alt="7f04b849747ae328fad6cd5e63fa9cf"></p>
<p><img src="/../blogimg/4ea5917696b1ce3a8d9a54eaec47997.jpg" alt="4ea5917696b1ce3a8d9a54eaec47997"></p>
<p><img src="/../blogimg/image-20250317205436167.png" alt="image-20250317205436167"></p>
</article></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/touxiang1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Vane</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/V-Ane"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">论文精读的内容并没有打上翻译，更多的是我对这一段的总结概括和理解</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Introduction"><span class="toc-number">2.</span> <span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Unified-Detection"><span class="toc-number">3.</span> <span class="toc-text">2. Unified Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Network-Design"><span class="toc-number">3.1.</span> <span class="toc-text">2.1. Network Design</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Training"><span class="toc-number">3.2.</span> <span class="toc-text">2.2. Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Inference"><span class="toc-number">3.3.</span> <span class="toc-text">2.3. Inference</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-Limitations-of-YOLO"><span class="toc-number">3.4.</span> <span class="toc-text">2.4. Limitations of YOLO</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Comparison-to-Other-Detection-Systems"><span class="toc-number">4.</span> <span class="toc-text">3. Comparison to Other Detection Systems</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Experiments"><span class="toc-number">5.</span> <span class="toc-text">4. Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-Comparison-to-Other-Real-Time-Systems"><span class="toc-number">5.1.</span> <span class="toc-text">4.1. Comparison to Other Real-Time Systems</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-VOC-2007-Error-Analysis"><span class="toc-number">5.2.</span> <span class="toc-text">4.2. VOC 2007 Error Analysis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-Combining-Fast-R-CNN-and-YOLO"><span class="toc-number">5.3.</span> <span class="toc-text">4.3. Combining Fast R-CNN and YOLO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-VOC-2012-Results"><span class="toc-number">5.4.</span> <span class="toc-text">4.4. VOC 2012 Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-Generalizability-Person-Detection-in-Artwork"><span class="toc-number">5.5.</span> <span class="toc-text">4.5. Generalizability: Person Detection in Artwork</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Real-Time-Detection-In-The-Wild"><span class="toc-number">6.</span> <span class="toc-text">5. Real-Time Detection In The Wild</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Conclusion"><span class="toc-number">7.</span> <span class="toc-text">6. Conclusion</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%B8%E8%B4%A8%E7%89%88%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95"><span class="toc-number">8.</span> <span class="toc-text">纸质版阅读记录</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/27/Faster%20R-CNN%E6%8A%80%E6%9C%AF/" title="Faster R-CNN 技术剖析">Faster R-CNN 技术剖析</a><time datetime="2025-03-27T09:03:57.097Z" title="发表于 2025-03-27 17:03:57">2025-03-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/26/Fast%20R-CNN%E6%8A%80%E6%9C%AF/" title="Fast R-CNN 技术剖析">Fast R-CNN 技术剖析</a><time datetime="2025-03-26T12:13:41.186Z" title="发表于 2025-03-26 20:13:41">2025-03-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/21/R-CNN%E7%AE%97%E6%B3%95/" title="R-CNN 技术剖析">R-CNN 技术剖析</a><time datetime="2025-03-21T12:03:51.550Z" title="发表于 2025-03-21 20:03:51">2025-03-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/18/ResNet%20-%20%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/" title="ResNet 代码实现(Pytorch)">ResNet 代码实现(Pytorch)</a><time datetime="2025-03-18T08:08:41.323Z" title="发表于 2025-03-18 16:08:41">2025-03-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/18/ResNet%20-%20%E8%AE%BA%E6%96%87/" title="ResNet 论文精读">ResNet 论文精读</a><time datetime="2025-03-18T06:34:50.734Z" title="发表于 2025-03-18 14:34:50">2025-03-18</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>